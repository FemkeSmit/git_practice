{
split_values <- sample.split(sev_new, SplitRatio = 0.65)
train_set <- subset(Data_new, split_values == T)
test_set <- subset(Data_new, split_values == F)
train_set <- na.omit(train_set)
test_set <- na.omit(test_set)
####Variables are not the same length, need to scale. How?
### building linear model ####
#set.seed(1)
#print(model)
#mod_regress <- lm(sev2 ~ ., data = train_set)
mod_regress <- plsr(sev2 ~ ., data=train_set, scale=TRUE, validation="CV", ncomp = 3)
result_regress <- predict(mod_regress, test_set, ncomp = 3)
Final_Data <- cbind(Actual=test_set$sev2, Predicted=result_regress)
Final_Data <- as.data.frame(Final_Data)
Final_Data$Actual <- test_set$sev2
#mean(result_regress == train_set$sev2)
### finding error ###
error <- (Final_Data$impact - Final_Data$Predicted)
Final_Data <- cbind(Final_Data, error)
rmse <- sqrt(mean(Final_Data$error^2))
rmse
#plot(x=Final_Data$Predicted, y=Final_Data$impact,
#     xlab='Predicted Values',
#     ylab='Actual Values',
#     main='Predicted vs. Actual Values')
#add diagonal line for estimated regression line
#abline(a=0, b=1)
#sqrt(mean((result_regress2-bla)^2))
#bla <-t(test_set$sev2)
#result_regress2 <- as.numeric(drop(result_regress))
return(list(error_vector))
}
)
error_vector <- replicate(
5,
{
split_values <- sample.split(sev_new, SplitRatio = 0.65)
train_set <- subset(Data_new, split_values == T)
test_set <- subset(Data_new, split_values == F)
train_set <- na.omit(train_set)
test_set <- na.omit(test_set)
####Variables are not the same length, need to scale. How?
### building linear model ####
#set.seed(1)
#print(model)
#mod_regress <- lm(sev2 ~ ., data = train_set)
mod_regress <- plsr(sev2 ~ ., data=train_set, scale=TRUE, validation="CV", ncomp = 3)
result_regress <- predict(mod_regress, test_set, ncomp = 3)
Final_Data <- cbind(Actual=test_set$sev2, Predicted=result_regress)
Final_Data <- as.data.frame(Final_Data)
Final_Data$Actual <- test_set$sev2
#mean(result_regress == train_set$sev2)
### finding error ###
error <- (Final_Data$impact - Final_Data$Predicted)
Final_Data <- cbind(Final_Data, error)
rmse <- sqrt(mean(Final_Data$error^2))
rmse
#plot(x=Final_Data$Predicted, y=Final_Data$impact,
#     xlab='Predicted Values',
#     ylab='Actual Values',
#     main='Predicted vs. Actual Values')
#add diagonal line for estimated regression line
#abline(a=0, b=1)
#sqrt(mean((result_regress2-bla)^2))
#bla <-t(test_set$sev2)
#result_regress2 <- as.numeric(drop(result_regress))
}
)
error_vector <- replicate(
500,
{
split_values <- sample.split(sev_new, SplitRatio = 0.65)
train_set <- subset(Data_new, split_values == T)
test_set <- subset(Data_new, split_values == F)
train_set <- na.omit(train_set)
test_set <- na.omit(test_set)
####Variables are not the same length, need to scale. How?
### building linear model ####
#set.seed(1)
#print(model)
#mod_regress <- lm(sev2 ~ ., data = train_set)
mod_regress <- plsr(sev2 ~ ., data=train_set, scale=TRUE, validation="CV", ncomp = 3)
result_regress <- predict(mod_regress, test_set, ncomp = 3)
Final_Data <- cbind(Actual=test_set$sev2, Predicted=result_regress)
Final_Data <- as.data.frame(Final_Data)
Final_Data$Actual <- test_set$sev2
#mean(result_regress == train_set$sev2)
### finding error ###
error <- (Final_Data$impact - Final_Data$Predicted);
Final_Data <- cbind(Final_Data, error);
rmse <- sqrt(mean(Final_Data$error^2));
rmse;
#plot(x=Final_Data$Predicted, y=Final_Data$impact,
#     xlab='Predicted Values',
#     ylab='Actual Values',
#     main='Predicted vs. Actual Values')
#add diagonal line for estimated regression line
#abline(a=0, b=1)
#sqrt(mean((result_regress2-bla)^2))
#bla <-t(test_set$sev2)
#result_regress2 <- as.numeric(drop(result_regress))
}
)
mean(error_vector)
install.packages("randomForest")
library(randomForest)
rf_bird_model <- randomForest(factor(sev2) ~ .,
data = train_set,
importance = TRUE)
print(rf_bird_model)
important <- importance(rf_bird_model)
plot(rf_bird_model)    #Plotting to find optimal amount of trees
err <- rf_bird_model$err.rate
oob_err <- err[nrow(err), "OOB"]
legend(x = "right",
legend = colnames(err),
fill = 1:ncol(err))
important
rf_predict <- predict(rf_bird_model,   # model object
newdata = test_set,  # test dataset
type = "class")
rf_predict_test <- predict(rf_bird_model,   # model object
newdata = train_set,  # train dataset
type = "class")
cm <- confusionMatrix(data = rf_predict,       # predicted classes
reference = test_set$sev2)  # actual classes
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(train_set$sev2))  # actual classes
cm <- confusionMatrix(data = rf_predict,       # predicted classes
reference = train_set$sev2)
dim(rf_predict)
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(test_set$sev2))
rf_predict <- predict(rf_bird_model,   # model object
newdata = train_set,  # test dataset
type = "class")
rf_predict_test <- predict(rf_bird_model,   # model object
newdata = test_set,  # train dataset
type = "class")
cm <- confusionMatrix(data = rf_predict,       # predicted classes
reference = train_set$sev2)  # actual classes
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(test_set$sev2))
print(cm2)
View(train_set)
rf_predict <- predict(rf_bird_model,   # model object
newdata = test_set,  # test dataset
type = "class")
rf_predict_test <- predict(rf_bird_model,   # model object
newdata = train_set,  # train dataset
type = "class")
cm <- confusionMatrix(data = rf_predict,       # predicted classes
reference = train_set$sev2)
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(test_set$sev2))
cm <- confusionMatrix(data = rf_predict,       # predicted classes
reference = test_set$sev2)
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(train_set$sev2))
print(cm2)
auc <- auc(test_set$sev2,
as.numeric(rf_predict))
auc
auc <- auc(test_set$sev2,
as.numeric(rf_predict))
library(Metrics)
auc <- auc(test_set$sev2,
as.numeric(rf_predict))
auc
rf_predict <- predict(rf_bird_model,   # model object
newdata = train_set,  # test dataset
type = "class")
auc <- auc(test_set$sev2,
as.numeric(rf_predict))
auc <- auc(train_set$sev2,
as.numeric(rf_predict))
auc
auc <- auc(test_set$sev2,
as.numeric(rf_predict))
rf_predict <- predict(rf_bird_model,   # model object
newdata = test_set,  # test dataset
type = "class")
rf_predict_test <- predict(rf_bird_model,   # model object
newdata = train_set,  # train dataset
type = "class")
auc <- auc(test_set$sev2,
as.numeric(rf_predict))
auc
print(cm)
cm <- confusionMatrix(data = rf_predict,       # predicted classes
reference = test_set$sev2)  # actual classes
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(train_set$sev2))
cm <- confusionMatrix(table(data = rf_predict,       # predicted classes
reference = test_set$sev2))
print(cm)
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
data <- read.delim("Data_tocheck.txt",header = FALSE)
setwd("C:\Users\rbrad\git_practice")
setwd("C:\\Users\\rbrad\\git_practice")
data <- read.delim("Data_tocheck.txt",header = FALSE)
dat <- data
cv_one <- 5
cv_two <- 10
scaling <- "autoscaling"
source("autoscaling.R")
source("Mean_center.R")
source("paretofunc.R")
results <- vector('list',cv_one)
trainIndex <- caret::createDataPartition(1:nrow(data),times = cv_one,p = 0.8)
#inner crossvalidation
for (i in 1:length(trainIndex)){
trainSet <- data[trainIndex[[i]],]
testSet <- data[-trainIndex[[i]],]
trainCVIndex <- caret::createDataPartition(1:nrow(trainSet),times = cv_two,p = 0.8)
for (ii in 1:length(trainCVIndex)){
trainSet2 <- trainSet[trainCVIndex[[ii]],]
testSet2 <- trainSet[-trainCVIndex[[ii]],]
#Scaling the test sets
if (scaling == 'autoscaling'){
scaled <- autoscaling(trainSet2)
testSet2Scaled <- sweep(testSet2,2,scaled[[2]],'-')
testSet2Scaled <- sweep(testSet2Scaled,2,scaled[[3]],'/')
testSetScaled <- sweep(testSet,2,scaled[[2]],'-')
testSetScaled <- sweep(testSetScaled,2,scaled[[3]],'/')
}
if (scaling == "mean_center"){
scaled <- center_colmeans(trainSet2)
}
if (scaling == "pareto"){
scaled <- paretoscaling(trainSet2)
}
#scaling training set
trainSet2Scaled <- scaled[[1]]
#THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
}
#THIS IS WHERE COMPARAISON WITH INDEPENDENT TEST SET WOULD HAPPEN
}
?sweep
View(scaled)
View(testSet2)
View(scaled)
center_sweep <- function(Data, row.w = rep(1, nrow(Data))/nrow(Data)) {
get_average <- function(v) sum(v * row.w)/sum(row.w)
average <- apply(Data, 2, get_average)
sweep(x, 2, average)
}
center_sweep(Data)
center_sweep <- function(Data, row.w = rep(1, nrow(Data))/nrow(Data)) {
get_average <- function(v) sum(v * row.w)/sum(row.w)
average <- apply(Data, 2, get_average)
sweep(Data, 2, average)
}
center_sweep <- function(x, row.w = rep(1, nrow(x))/nrow(x)) {
get_average <- function(v) sum(v * row.w)/sum(row.w)
average <- apply(x, 2, get_average)
sweep(x, 2, average)
}
center_sweep(Data)
center_colmeans <- function(x) {
xcenter = colMeans(x)
x - rep(xcenter, rep.int(nrow(x), ncol(x)))
}
center_colmeans(Data)
?replications
?rep
center_sweep <- function(x, row.w = rep(1, nrow(x))/nrow(x)) {
get_average <- function(v) sum(v * row.w)/sum(row.w)
average <- apply(x, 2, get_average)
sweep(x, 2, average)
}
center_sweep(Data)
View(autoscaling)
autoscaling(trainSet2)
testSet2Scaled <- center_sweep(testSet)
colmean(testSet2Scaled)
mean(testSet2Scaled)
View(testSet2Scaled)
if (scaling == "mean_center"){
scaled <- center_sweep(trainSet2)
testSet2Scaled <- center_sweep(testSet2)
testSet2Scaled <- center_sweep(testSet)
}
colMeans(Data[sapply(Data, is.numeric)])
center_sweep(Data)
colMeans(Data[sapply(Data, is.numeric)])
mean(Data[,3])
mean(Data[,1])
Get_average(Data[sapply(Data, is.numeric)])
center_sweep <- function(x, row.w = rep(1, nrow(x))/nrow(x)) {
col_means <- function(v) sum(v * row.w)/sum(row.w)
average <- apply(x, 2, col_means)
sweep(x, 2, average, '-')
}
center_sweep(Data)
center_sweep <- function(x, row.w = rep(1, nrow(x))/nrow(x)) {
col_means <- function(v) sum(v * row.w)/sum(row.w)
average <- apply(x, 2, col_means)
sweep(x, 2, average, '+')
}
center_sweep(Data)
colMeans(Data[sapply(Data, is.numeric)])
mean(Data[,2])
?row.w
??row.w
?rep
autoscaling(Data)
Data1 <- autoscaling(Data)
Data2 <- center_sweep(Data1)
Data1
Data2 <- center_sweep(Data1$matrix)
mean(Data2[,2])
mean(Data2[,1])
mean(Data2[,3])
center_sweep <- function(x, row.w = rep(1, nrow(x))/nrow(x)) {
col_means <- function(v) sum(v * row.?rw)/sum(row.w)
average <- apply(x, 2, col_means)
sweep(x, 2, average)
}
#scaling training set
trainSet2Scaled <- scaled[[1]]
#THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
}
View(trainSet2)
if (scaling == "mean_center"){
scaled <- center_sweep(trainSet2)
testSet2Scaled <- center_sweep(testSet2)
testSet2Scaled <- center_sweep(testSet)
#       }
#       if (scaling == "pareto"){
#         scaled <- paretoscaling(trainSet2)
#       }
#       #scaling training set
#       trainSet2Scaled <- scaled[[1]]
#
#       #THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
#     }
#     #THIS IS WHERE COMPARAISON WITH INDEPENDENT TEST SET WOULD HAPPEN
#   }
#   #THIS IS WHERE OVERALL RESULTS WOULD BE ASSESSED
# #}
}
View(trainSet2Scaled)
for (ii in 1:length(trainCVIndex)){
trainSet2 <- trainSet[trainCVIndex[[ii]],]
testSet2 <- trainSet[-trainCVIndex[[ii]],]
#Scaling the test sets
if (scaling == 'autoscaling'){
scaled <- autoscaling(trainSet2)
testSet2Scaled <- sweep(testSet2,2,scaled[[2]],'-')
testSet2Scaled <- sweep(testSet2Scaled,2,scaled[[3]],'/')
testSetScaled <- sweep(testSet,2,scaled[[2]],'-')
testSetScaled <- sweep(testSetScaled,2,scaled[[3]],'/')
}
if (scaling == "mean_center"){
scaled <- center_sweep(trainSet2)
testSet2Scaled <- center_sweep(testSet2)
testSet2Scaled <- center_sweep(testSet)
#       }
#       if (scaling == "pareto"){
#         scaled <- paretoscaling(trainSet2)
#       }
#       #scaling training set
#       trainSet2Scaled <- scaled[[1]]
#
#       #THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
#     }
#     #THIS IS WHERE COMPARAISON WITH INDEPENDENT TEST SET WOULD HAPPEN
#   }
#   #THIS IS WHERE OVERALL RESULTS WOULD BE ASSESSED
# #}
}
}
if (scaling == "mean_center"){
scaled <- center_sweep(trainSet2)
testSet2Scaled <- center_sweep(testSet2)
testSet2Scaled <- center_sweep(testSet)
#       }
#       if (scaling == "pareto"){
#         scaled <- paretoscaling(trainSet2)
#       }
#       #scaling training set
#       trainSet2Scaled <- scaled[[1]]
#
#       #THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
#     }
#     #THIS IS WHERE COMPARAISON WITH INDEPENDENT TEST SET WOULD HAPPEN
#   }
#   #THIS IS WHERE OVERALL RESULTS WOULD BE ASSESSED
# #}
}
mean(testSet2Scaled[,1])
mean(testSet2Scaled[,2])
for (ii in 1:length(trainCVIndex)){
trainSet2 <- trainSet[trainCVIndex[[ii]],]
testSet2 <- trainSet[-trainCVIndex[[ii]],]
#Scaling the test sets
if (scaling == 'autoscaling'){
scaled <- autoscaling(trainSet2)
testSet2Scaled <- sweep(testSet2,2,scaled[[2]],'-')
testSet2Scaled <- sweep(testSet2Scaled,2,scaled[[3]],'/')
testSetScaled <- sweep(testSet,2,scaled[[2]],'-')
testSetScaled <- sweep(testSetScaled,2,scaled[[3]],'/')
}
if (scaling == "mean_center"){
scaled <- center_sweep(trainSet2)
testSet2Scaled <- center_sweep(testSet2)
testSet2Scaled <- center_sweep(testSet)
#       }
#       if (scaling == "pareto"){
#         scaled <- paretoscaling(trainSet2)
#       }
#       #scaling training set
#       trainSet2Scaled <- scaled[[1]]
#
#       #THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
#     }
#     #THIS IS WHERE COMPARAISON WITH INDEPENDENT TEST SET WOULD HAPPEN
#   }
#   #THIS IS WHERE OVERALL RESULTS WOULD BE ASSESSED
# #}
}
}
mean(testSetScaled[,1])
mean(testSet2Scaled[,2])
mean(testSetScaled[,2])
mean(scaled[,1])
View(scaled)
mean(scaled$matrix[,1])
mean(scaled$matrix[,2])
print(cm)
print(cm2)
cm <- confusionMatrix(table(data = rf_predict,       # predicted classes
reference = test_set$sev2))  # actual classes
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(train_set$sev2))
print(cm)
print(cm2)
?predict
rf_predict <- predict(rf_bird_model,   # model object
newdata = test_set,  # test dataset
type = "response")
rf_predict_test <- predict(rf_bird_model,   # model object
newdata = train_set,  # train dataset
type = "response")
cm <- confusionMatrix(table(data = rf_predict,       # predicted classes
reference = test_set$sev2))  # actual classes
cm2 <- confusionMatrix(factor(rf_predict_test),       # predicted classes
factor(train_set$sev2))
print(cm)
print(cm2)
paste0("Test Accuracy: ", cm$overall[1])
paste0("OOB Accuracy: ", 1 - oob_err)
library(Metrics)
auc <- auc(test_set$sev2,
as.numeric(rf_predict))
auc
?randomForest
View(testSet)
View(train_set)
#--------------------#
#   Options to set   #
#--------------------#
dat <- data
if (scaling == "mean_center"){
scaled <- center_sweep(trainSet2)
testSet2Scaled <- center_sweep(testSet2)
testSetScaled <- center_sweep(trainSet2)
}
mean(scaled$matrix[,1])
mean(testSet2[,1])
mean(testSet2Scaled[,1])
mean(testSetScaled[,1])
#THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
}
#THIS IS WHERE COMPARAISON WITH INDEPENDENT TEST SET WOULD HAPPEN
}
#inner crossvalidation
for (i in 1:length(trainIndex)){
trainSet <- data[trainIndex[[i]],]
testSet <- data[-trainIndex[[i]],]
trainCVIndex <- caret::createDataPartition(1:nrow(trainSet),times = cv_two,p = 0.8)
for (ii in 1:length(trainCVIndex)){
trainSet2 <- trainSet[trainCVIndex[[ii]],]
testSet2 <- trainSet[-trainCVIndex[[ii]],]
#Scaling the test sets
if (scaling == 'autoscaling'){
scaled <- autoscaling(trainSet2)
testSet2Scaled <- sweep(testSet2,2,scaled[[2]],'-')
testSet2Scaled <- sweep(testSet2Scaled,2,scaled[[3]],'/')
testSetScaled <- sweep(testSet,2,scaled[[2]],'-')
testSetScaled <- sweep(testSetScaled,2,scaled[[3]],'/')
}
if (scaling == "mean_center"){
scaled <- center_sweep(trainSet2)
testSet2Scaled <- center_sweep(testSet2)
testSetScaled <- center_sweep(trainSet2)
}
if (scaling == "pareto"){
scaled <- paretoscaling(trainSet2)
testSet2Scaled <- paretoscaling(testSet2)
testSetScaled <- paretoscaling(trainSet2)
}
#scaling training set
trainSet2Scaled <- scaled[[1]]
#THIS IS WHERE TRAINING AND INNER CROSS VALIDATION WOULD HAPPEN
#THIS IS WHERE COMPARAISON WITH INDEPENDENT TEST SET WOULD HAPPEN
}
#THIS IS WHERE OVERALL RESULTS WOULD BE ASSESSED
}
